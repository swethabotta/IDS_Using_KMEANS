# -*- coding: utf-8 -*-
"""KMeans_Withoutpca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lbmu6BV12Lgx7PN28qxwnxXfD0RBkrec
"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import silhouette_score
import time

# Load the dataset
Train_data = pd.read_csv("/content/KDDTrain+.txt")
Test_data = pd.read_csv("/content/KDDTest+.txt")

columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level']

Train_data.columns = columns
Test_data.columns = columns

# Select relevant features
selected_features = ['protocol_type', 'attack', 'src_bytes', 'dst_bytes']
data_selected = Train_data[selected_features]

# Convert 'attack' column to binary (normal = 0, anomaly = 1) using .loc to avoid SettingWithCopyWarning
data_selected.loc[:, "attack"] = data_selected.attack.apply(lambda x: 0 if x == "normal" else 1)

# One-hot encode categorical features
ohe = OneHotEncoder(sparse=False)
data_encoded = pd.get_dummies(data_selected, columns=['protocol_type'])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_encoded.drop(columns='attack'), data_encoded['attack'], test_size=0.2, random_state=42)

# Normalize numerical features
scaler = StandardScaler()
X_train_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.transform(X_test)

# Hyperparameter tuning for KMeans
param_distributions = {
    'n_clusters': [3, 4, 5, 6],
    'init': ['k-means++', 'random'],
    'n_init': [10, 20],
    'max_iter': [200, 500],
    'tol': [1e-4, 1e-3]
}

kmeans = KMeans(random_state=42)
random_search = RandomizedSearchCV(kmeans, param_distributions, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, n_iter=10, random_state=42)
start_time = time.time()
random_search.fit(X_train_normalized)
end_time = time.time()
print(f"Time taken for hyperparameter tuning: {end_time - start_time:.6f} seconds")

print("Best hyperparameters:", random_search.best_params_)
best_kmeans = random_search.best_estimator_

# Fit the KMeans model
best_kmeans.fit(X_train_normalized)

# Predict clusters for training and testing data
train_clusters = best_kmeans.predict(X_train_normalized)
test_clusters = best_kmeans.predict(X_test_normalized)

# Create separate data frames for normal and anomaly data
train_data_df = pd.DataFrame(X_train_normalized, columns=[f'Feature {i+1}' for i in range(X_train_normalized.shape[1])])
train_data_df['Cluster'] = train_clusters
normal_train_data = train_data_df[train_data_df['Cluster'] == 0].drop(columns='Cluster')
anomaly_train_data = train_data_df[train_data_df['Cluster'] == 1].drop(columns='Cluster')

test_data_df = pd.DataFrame(X_test_normalized, columns=[f'Feature {i+1}' for i in range(X_test_normalized.shape[1])])
test_data_df['Cluster'] = test_clusters
normal_test_data = test_data_df[test_data_df['Cluster'] == 0].drop(columns='Cluster')
anomaly_test_data = test_data_df[test_data_df['Cluster'] == 1].drop(columns='Cluster')

# Print anomalies
anomalies_train = X_train[y_train == 1]
anomalies_test = X_test[y_test == 1]

print("Anomalies in Training Data:")
print(anomalies_train)

print("\nAnomalies in Testing Data:")
print(anomalies_test)


from sklearn.metrics import confusion_matrix, accuracy_score

# Predict clusters for testing data
test_clusters_pred = best_kmeans.predict(X_test_normalized)

# Convert the attack column to binary (normal = 0, anomaly = 1) for y_test
y_true = y_test.apply(lambda x: 0 if x == "normal" else 1)

# Convert cluster labels to binary (normal = 0, anomaly = 1) for y_pred
y_pred = pd.Series(test_clusters_pred).apply(lambda x: 0 if x == 0 else 1)

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)

# Calculate accuracy score
accuracy = accuracy_score(y_true, y_pred)
print("\nAccuracy:", accuracy)

plt.figure(figsize=(8, 6))
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.colorbar()
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from sklearn.metrics import recall_score, precision_score, classification_report,f1_score
recall = recall_score(y_true, y_pred)
print("Recall:", recall)

# Calculate precision score
precision = precision_score(y_true, y_pred)
print("Precision:", precision)

# Calculate F1-score
f1 = f1_score(y_true, y_pred)
print("F1-score:", f1)

print("\nClassification Report:")
print(classification_report(y_true, y_pred))

print("\nNormal in Train_Data")
normal_train = X_train[y_train==0]
print(normal_train)

print("\n Normal in test data")
normal_test = X_test[y_test==0]
print(normal_test)

