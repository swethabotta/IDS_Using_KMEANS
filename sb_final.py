# -*- coding: utf-8 -*-
"""SB_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Q_RzKxtx2Xlpou5aun_D8ISAxINSUcW
"""

import pandas as pd
Train_data=pd.read_csv('/content/KDDTrain.csv')
print(Train_data.shape)

Test_data=pd.read_csv('/content/KDDTest+.txt')
print(Test_data.shape)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import silhouette_score, confusion_matrix, accuracy_score, f1_score, precision_score, classification_report, make_scorer
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import time
import sys
import seaborn as sns

def load_data(file_path):
    return pd.read_csv(file_path, header=None, names=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'])

def preprocess_data(data):
    X = data[["src_bytes", "dst_bytes", "protocol_type"]].copy()
    X.loc[:, 'protocol_type'] = LabelEncoder().fit_transform(X['protocol_type'])
    scaler = MinMaxScaler()
    X.loc[:, ['src_bytes', 'dst_bytes']] = scaler.fit_transform(X[['src_bytes', 'dst_bytes']])
    return X

def silhouette_scorer(X, labels):
    return silhouette_score(X, labels)

silhouette_scorer = make_scorer(silhouette_scorer, needs_proba=False)

def train_evaluate_model(X_train, X_test, y_train, y_test, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(X_train)
    y_pred = kmeans.predict(X_test)
    train_score = silhouette_score(X_train, kmeans.labels_)
    test_score = silhouette_score(X_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='binary')
    precision = precision_score(y_test, y_pred, average='binary')
    report = classification_report(y_test, y_pred)
    return kmeans, train_score, test_score, conf_matrix, accuracy, f1, precision, report

def calculate_complexity(train_data, X_train, X_test, kmeans):
    space_complexity = {
        'train_data': sys.getsizeof(train_data),
        'X_train': sys.getsizeof(X_train),
        'X_test': sys.getsizeof(X_test),
        'kmeans': sys.getsizeof(kmeans)
    }
    total_space_complexity = sum(space_complexity.values())
    return space_complexity, total_space_complexity

def train_and_evaluate(file_path_train, file_path_test=None, split_data=True, use_pca=False):
    start_time = time.time()

    # Loading training data
    train_data = load_data(file_path_train)

    # Preprocessing training data
    X_train = preprocess_data(train_data)
    y_train = train_data['attack'].apply(lambda x: 0 if x == "normal" else 1)

    # Applying PCA if specified
    if use_pca:
        pca = PCA(n_components=2)
        X_train = pca.fit_transform(X_train)

    if not split_data:
        # Loading testing data
        test_data = load_data(file_path_test)
        X_test = preprocess_data(test_data)
        y_test = test_data['attack'].apply(lambda x: 0 if x == "normal" else 1)

        # Applying PCA if specified
        if use_pca:
            X_test = pca.transform(X_test)
    else:
        # Splitting data into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    # Defining parameter grid for hyperparameter tuning
    param_grid = {'n_clusters': range(2, 11)}

    # Performing hyperparameter tuning
    kmeans_grid = GridSearchCV(KMeans(random_state=42), param_grid, scoring=silhouette_scorer, cv=3, n_jobs=-1)
    kmeans_grid.fit(X_train)

    # Getting the best hyperparameters
    best_params = kmeans_grid.best_params_
    best_n_clusters = best_params['n_clusters']

    # Training and evaluating model with the best hyperparameters
    kmeans, train_score, test_score, conf_matrix, accuracy, f1, precision, report = train_evaluate_model(X_train, X_test, y_train, y_test, best_n_clusters)

    # Calculating complexity
    space_complexity, total_space_complexity = calculate_complexity(train_data, X_train if split_data else X_train, X_test, kmeans)

    # Printing results
    print(f"Best Hyperparameters: {best_params}")
    print(f"Number of Clusters: {best_n_clusters}")
    print(f"Silhouette Score (Train): {train_score:.2f}")
    print(f"Silhouette Score (Test): {test_score:.2f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print(f"\nAccuracy: {accuracy:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print(f"Precision: {precision:.2f}")
    print("\nClassification Report:")
    print(report)
    for obj, size in space_complexity.items():
        print(f"Space Complexity of {obj}: {size} bytes")
    print(f"Total Space Complexity: {total_space_complexity} bytes")

    # Plotting the confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Recording end time
    end_time = time.time()

    # Calculating execution time
    execution_time = end_time - start_time
    print("Execution Time:", execution_time, "seconds")

if __name__ == "__main__":
    option_split = input("Do you want to split the data into train and test sets? (yes/no): ").lower()
    option_pca = input("Do you want to use PCA? (yes/no): ").lower()

    if option_split == "yes":
        if option_pca == "yes":
            train_and_evaluate("/content/KDDTrain+.txt", split_data=True, use_pca=True)
        elif option_pca == "no":
            train_and_evaluate("/content/KDDTrain+.txt", split_data=True, use_pca=False)
        else:
            print("Invalid option for PCA! Please enter 'yes' or 'no'.")
    elif option_split == "no":
        if option_pca == "yes":
            train_and_evaluate("/content/KDDTrain+.txt", file_path_test="/content/KDDTest+.txt", split_data=False, use_pca=True)
        elif option_pca == "no":
            train_and_evaluate("/content/KDDTrain+.txt", file_path_test="/content/KDDTest+.txt", split_data=False, use_pca=False)
        else:
            print("Invalid option for PCA! Please enter 'yes' or 'no'.")
    else:
        print("Invalid option for splitting! Please enter 'yes' or 'no'.")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import silhouette_score, confusion_matrix, accuracy_score, f1_score, precision_score, classification_report, make_scorer
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import time
import sys
import seaborn as sns

def load_data(file_path):
    return pd.read_csv(file_path, header=None, names=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'])

def preprocess_data(data):
    X = data[["src_bytes", "dst_bytes", "protocol_type"]].copy()
    X.loc[:, 'protocol_type'] = LabelEncoder().fit_transform(X['protocol_type'])
    scaler = MinMaxScaler()
    X.loc[:, ['src_bytes', 'dst_bytes']] = scaler.fit_transform(X[['src_bytes', 'dst_bytes']])
    return X

def silhouette_scorer(X, labels):
    return silhouette_score(X, labels)

silhouette_scorer = make_scorer(silhouette_scorer, needs_proba=False)

def train_evaluate_model(X_train, X_test, y_train, y_test, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(X_train)
    y_pred = kmeans.predict(X_test)
    train_score = silhouette_score(X_train, kmeans.labels_)
    test_score = silhouette_score(X_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='binary')
    precision = precision_score(y_test, y_pred, average='binary')
    report = classification_report(y_test, y_pred)
    return kmeans, train_score, test_score, conf_matrix, accuracy, f1, precision, report

def calculate_complexity(train_data, X_train, X_test, kmeans):
    space_complexity = {
        'train_data': sys.getsizeof(train_data),
        'X_train': sys.getsizeof(X_train),
        'X_test': sys.getsizeof(X_test),
        'kmeans': sys.getsizeof(kmeans)
    }
    total_space_complexity = sum(space_complexity.values())
    return space_complexity, total_space_complexity

def train_and_evaluate(file_path_train, file_path_test=None, split_data=True, use_pca=False):
    start_time = time.time()

    # Loading training data
    train_data = load_data(file_path_train)

    # Preprocessing training data
    X_train = preprocess_data(train_data)
    y_train = train_data['attack'].apply(lambda x: 0 if x == "normal" else 1)

    # Applying PCA if specified
    if use_pca:
        pca = PCA(n_components=2)
        X_train = pca.fit_transform(X_train)

    if not split_data:
        # Loading testing data
        test_data = load_data(file_path_test)
        X_test = preprocess_data(test_data)
        y_test = test_data['attack'].apply(lambda x: 0 if x == "normal" else 1)

        # Applying PCA if specified
        if use_pca:
            X_test = pca.transform(X_test)
    else:
        # Splitting data into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    # Defining parameter grid for hyperparameter tuning
    param_grid = {'n_clusters': range(2, 11)}

    # Performing hyperparameter tuning
    kmeans_grid = GridSearchCV(KMeans(random_state=42), param_grid, scoring=silhouette_scorer, cv=3, n_jobs=-1)
    kmeans_grid.fit(X_train)

    # Getting the best hyperparameters
    best_params = kmeans_grid.best_params_
    best_n_clusters = best_params['n_clusters']

    # Training and evaluating model with the best hyperparameters
    kmeans, train_score, test_score, conf_matrix, accuracy, f1, precision, report = train_evaluate_model(X_train, X_test, y_train, y_test, best_n_clusters)

    # Calculating complexity
    space_complexity, total_space_complexity = calculate_complexity(train_data, X_train if split_data else X_train, X_test, kmeans)

    # Printing results
    print(f"Best Hyperparameters: {best_params}")
    print(f"Number of Clusters: {best_n_clusters}")
    print(f"Silhouette Score (Train): {train_score:.2f}")
    print(f"Silhouette Score (Test): {test_score:.2f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print(f"\nAccuracy: {accuracy:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print(f"Precision: {precision:.2f}")
    print("\nClassification Report:")
    print(report)
    for obj, size in space_complexity.items():
        print(f"Space Complexity of {obj}: {size} bytes")
    print(f"Total Space Complexity: {total_space_complexity} bytes")

    # Plotting the confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Recording end time
    end_time = time.time()

    # Calculating execution time
    execution_time = end_time - start_time
    print("Execution Time:", execution_time, "seconds")

if __name__ == "__main__":
    option_split = input("Do you want to split the data into train and test sets? (yes/no): ").lower()
    option_pca = input("Do you want to use PCA? (yes/no): ").lower()

    if option_split == "yes":
        if option_pca == "yes":
            train_and_evaluate("/content/KDDTrain+.txt", split_data=True, use_pca=True)
        elif option_pca == "no":
            train_and_evaluate("/content/KDDTrain+.txt", split_data=True, use_pca=False)
        else:
            print("Invalid option for PCA! Please enter 'yes' or 'no'.")
    elif option_split == "no":
        if option_pca == "yes":
            train_and_evaluate("/content/KDDTrain+.txt", file_path_test="/content/KDDTest+.txt", split_data=False, use_pca=True)
        elif option_pca == "no":
            train_and_evaluate("/content/KDDTrain+.txt", file_path_test="/content/KDDTest+.txt", split_data=False, use_pca=False)
        else:
            print("Invalid option for PCA! Please enter 'yes' or 'no'.")
    else:
        print("Invalid option for splitting! Please enter 'yes' or 'no'.")

